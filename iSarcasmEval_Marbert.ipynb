{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUiwkRv-XAkW"
      },
      "source": [
        "# Download MARBERT checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIUCz5eveSFi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8wBRd5zXAkX"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FYvgJevXAkY"
      },
      "outputs": [],
      "source": [
        "!tar -xvf MARBERT_pytorch_verison.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwxvO8pu1nu_"
      },
      "outputs": [],
      "source": [
        "!pip install GPUtil pytorch_pretrained_bert transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZehsgkL91j-M"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "from sklearn import *\n",
        "# (1)load libraries \n",
        "import json, sys, regex\n",
        "import torch\n",
        "import GPUtil\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import  BertTokenizer, BertConfig, BertForSequenceClassification\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import RandomState\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYh4RFPzb1Ai"
      },
      "outputs": [],
      "source": [
        "!mkdir -p  isarcastic\n",
        "!cp \"/gdrive/MyDrive/Master/Dataset/isarcastic/isarcastic_Ar_train_test/isarcastic_Ar_train.csv\"  ./isarcastic/isarcastic_Ar_train.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/gdrive/MyDrive/Master/Dataset/isarcastic/isarcastic_Ar_train_test/isarcastic_Ar_test1.csv\"  ./isarcastic/isarcastic_Ar_test1.csv\n",
        "!cp \"/gdrive/MyDrive/Master/Dataset/isarcastic/isarcastic_Ar_train_test/isarcastic_Ar_test2.csv\"  ./isarcastic/isarcastic_Ar_test2.csv"
      ],
      "metadata": {
        "id": "xJRGs4a3EyKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/gdrive/MyDrive/Master/Dataset/TestingData/task_A_AR_test.csv\"  ./isarcastic/task_A_AR_test.csv"
      ],
      "metadata": {
        "id": "5p7jAAeihp-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN50DNeGdUFQ"
      },
      "outputs": [],
      "source": [
        "file = open('isarcastic/task_A_AR_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCsdKpOddeFq"
      },
      "outputs": [],
      "source": [
        "csvreader = csv.reader(file)\n",
        "header = next(csvreader)\n",
        "print(header)\n",
        "rows = []\n",
        "for row in csvreader:\n",
        "    rows.append(row)\n",
        "print(rows)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBm7opGXAkb"
      },
      "source": [
        "# Fine-tuning code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQWKMrXPXAkc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"your device \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECMqsLVkhVcD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('isarcastic/task_A_AR_test.csv', sep=\",\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdPCPv8VXAke"
      },
      "outputs": [],
      "source": [
        "def flat_pred(preds, labels):\n",
        "\tpred_flat = np.argmax(preds, axis=1).flatten()\n",
        "\tlabels_flat = labels.flatten()\n",
        "\treturn pred_flat.tolist(), labels_flat.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vyvc8JoXAke"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "\t\n",
        "\tmodel.train()\n",
        "\tepoch_loss = 0\n",
        "\tfor i, batch in enumerate(iterator):\n",
        "\t\t# Add batch to GPU\n",
        "\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t# Unpack the inputs from our dataloader\n",
        "\t\tinput_ids, input_mask, labels = batch\n",
        "\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\tloss, logits = outputs[:2]\n",
        "\t\t'''all_pred_global.extend(outputs[1])\n",
        "\t\tprint(all_pred_global)'''\n",
        "\t\t# delete used variables to free GPU memory\n",
        "\t\tdel batch, input_ids, input_mask, labels\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\telse:\n",
        "\t\t\tloss.sum().backward()\n",
        "\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\toptimizer.step()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "\t\t# optimizer.step()\n",
        "\t\tscheduler.step()\n",
        "\t# free GPU memory\n",
        "\tif device == 'cuda':\n",
        "\t\ttorch.cuda.empty_cache()\n",
        "\treturn epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG91VRz56rei"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\tmodel.eval()\n",
        "\tepoch_loss = 0\n",
        "\tall_pred=[]\n",
        "\tall_label = []\n",
        "\twith torch.no_grad():\n",
        "\t\tfor i, batch in enumerate(iterator):\n",
        "\t\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t\tinput_ids, input_mask, labels = batch\n",
        "\t\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\tdel batch, input_ids, input_mask\n",
        "\t\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\t\telse:\n",
        "\t\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\t\tprobabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "\t\t\tall_pred.extend(predicted)\n",
        "\t\t\tall_label.extend(labels.cpu())\n",
        "\n",
        "\taccuracy = accuracy_score(all_label, all_pred)\n",
        "\tprint(\"all_pred\", all_pred)\n",
        "\tfor i in all_pred:\n",
        "\t\tprint(i.item())\n",
        "\tf1_sarcastic = f1_score(all_label, all_pred, average='binary', pos_label=1) \n",
        "\tf1score = f1_score(all_label, all_pred, average='macro') \n",
        "\trecall = recall_score(all_label, all_pred, average='macro')\n",
        "\tprecision = precision_score(all_label, all_pred, average='macro')\n",
        "\treport = classification_report(all_label, all_pred)\n",
        "\treturn (epoch_loss / len(iterator)), accuracy, f1score, recall, precision, f1_sarcastic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config"
      ],
      "metadata": {
        "id": "w5XvRB0mSkc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config={\"task_name\": \"isarcastic_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"checkpoint_dir\": \"/gdrive/MyDrive/Master/Dataset/isarcastic\",\n",
        "             \"train_file\": \"isarcastic/isarcastic_Ar_train.csv\", #train file path\n",
        "            #  \"dev_file\": \"isarcastic/task_A_AR_test.csv\", #dev file path or test file path\n",
        "              \"dev_file\": \"isarcastic/isarcastic_Ar_test1.csv\", #dev file path or test file path\n",
        "\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"fine_tuned_model_path\" : \"/gdrive/MyDrive/Master/Dataset/isarcasticisarcastic_MARBERT_bert_ckpt/model_5\",\n",
        "             \"epochs\": 5, #number of epochs\n",
        "             \"content_col\": \"tweet\", #text column\n",
        "            #  \"content_col\": \"text\",\n",
        "             \"label_col\": \"sarcastic\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 64 ,#128, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ],
      "metadata": {
        "id": "5dhPEBodSjmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDnDyOFHiZvk"
      },
      "source": [
        "# Run fine-tuning for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsaxucgpkZNP"
      },
      "outputs": [],
      "source": [
        "#---------------------------------------\n",
        "print (\"[INFO] step (1) load train_test config file\")\n",
        "# config_file = open(config_file, 'r', encoding=\"utf8\")\n",
        "# config = json.load(config_file)\n",
        "task_name = config[\"task_name\"]\n",
        "content_col = config[\"content_col\"]\n",
        "label_col = config[\"label_col\"]\n",
        "train_file = config[\"data_dir\"]+config[\"train_file\"]\n",
        "dev_file = config[\"data_dir\"]+config[\"dev_file\"]\n",
        "sortby = config[\"sortby\"]\n",
        "max_seq_length= int(config[\"max_seq_length\"])\n",
        "batch_size = int(config[\"batch_size\"])\n",
        "lr_var = float(config[\"lr\"])\n",
        "model_path = config['pretrained_model_path']\n",
        "num_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n",
        "global label2idx_file\n",
        "label2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n",
        "#-------------------------------------------------------\n",
        "print (\"[INFO] step (2) convert labels2index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O3AF4_kkmdg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_label2ind_file(file, label_col):\n",
        "\tlabels_json={}\n",
        "\t#load train_dev_test file\n",
        "\tdf = pd.read_csv(file, sep=\",\")\n",
        "\tdf.head(5)\n",
        "\t#get labels and sort it A-Z\n",
        "\tlabels = df[label_col].unique()\n",
        "\tlabels.sort()\n",
        "\t#convert labels to indexes\n",
        "\tfor idx in range(0, len(labels)):\n",
        "\t\tlabels_json[labels[idx]]=idx\n",
        "\t#save labels with indexes to file\n",
        "\t# with open(label2idx_file, 'w') as json_file:\n",
        "\t# \tjson.dump(labels_json, json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz2LPU-RjGke"
      },
      "outputs": [],
      "source": [
        "create_label2ind_file(train_file, label_col)\n",
        "print (label2idx_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7gXCLlMnIGh"
      },
      "outputs": [],
      "source": [
        "#---------------------------------------------------------\n",
        "print (\"[INFO] step (3) check checkpoit directory and report file\")\n",
        "ckpt_dir = config[\"checkpoint_dir\"]+task_name+\"_bert_ckpt/\"\n",
        "report = ckpt_dir+task_name+\"_report.tsv\"\n",
        "sorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.mkdir(ckpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsAf31i9nLDP"
      },
      "outputs": [],
      "source": [
        "\n",
        "#-------------------------------------------------------\n",
        "print (\"[INFO] step (4) load label to number dictionary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbYBrueXnacr"
      },
      "outputs": [],
      "source": [
        "x =  '{ \"0\":0, \"1\":1}'\n",
        "\n",
        "# parse x:\n",
        "y = json.loads(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5CVBGN_noLG"
      },
      "outputs": [],
      "source": [
        "lab2ind = json.loads(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9F2ow4eoYOR"
      },
      "outputs": [],
      "source": [
        "print (\"[INFO] train_file\", train_file)\n",
        "print (\"[INFO] dev_file\", dev_file)\n",
        "print (\"[INFO] num_epochs\", num_epochs)\n",
        "print (\"[INFO] model_path\", model_path)\n",
        "print (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n",
        "#-------------------------------------------------------\n",
        "print (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n",
        "print (\"loading BERT setting\")\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_file, delimiter=',', header=0)\n",
        "df = df[df[content_col].notnull()]\n",
        "df = df[df[label_col].notnull()]\n",
        "print(\"Data size \", df.shape)\n",
        "sentences = df[content_col].values\n",
        "sentences = [\"[CLS] \" + sentence+ \" [SEP]\" for sentence in sentences]\n",
        "print (\"The first sentence:\")\n",
        "print (sentences[0])\n",
        "labels = df[label_col].values\n",
        "print(labels)\n",
        "labels = [lab2ind[ str(i)] for i in labels]\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "metadata": {
        "id": "OPI8q5IFFiSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_ids)"
      ],
      "metadata": {
        "id": "SR94_0yKFwMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i) for i in input_ids]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "metadata": {
        "id": "qYumqVKXF5Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(seq_len)"
      ],
      "metadata": {
        "id": "c0rHCF1dGJcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZCJJep4oheo"
      },
      "outputs": [],
      "source": [
        "def data_prepare_BERT(file_path, lab2ind, tokenizer, content_col, label_col, MAX_LEN):\n",
        "  df = pd.read_csv(file_path, delimiter=',', header=0)\n",
        "  df = df[df[content_col].notnull()]\n",
        "  df = df[df[label_col].notnull()]\n",
        "  print(\"Data size \", df.shape)\n",
        "  sentences = df[content_col].values\n",
        "  sentences = [\"[CLS] \" + sentence+ \" [SEP]\" for sentence in sentences]\n",
        "  print (\"The first sentence:\")\n",
        "  print (sentences[0])\n",
        "  labels = df[label_col].values\n",
        "  print(labels)\n",
        "  labels = [lab2ind[ str(i)] for i in labels]\n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "  print (\"Tokenize the first sentence:\")\n",
        "  print (tokenized_texts[0])\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  print (\"Index numbers of the first sentence:\")\n",
        "  print (input_ids[0])\n",
        "  pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "  print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "  attention_masks = []\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [int(i > 0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "  inputs = torch.tensor(input_ids)\n",
        "  labels = torch.tensor(labels)\n",
        "  masks = torch.tensor(attention_masks)\n",
        "  return inputs, labels, masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeQWT1_Joa1a"
      },
      "outputs": [],
      "source": [
        "train_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V1EOq8_qstM"
      },
      "outputs": [],
      "source": [
        "validation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9u3dek-qw0N"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnalHQddq32b"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------\n",
        "print (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "#---------------------------\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
        "#------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpX1yhv3q9-i"
      },
      "outputs": [],
      "source": [
        "print (\"[INFO] step (7) run with parallel GPUs\")\n",
        "if torch.cuda.is_available():\n",
        "  if torch.cuda.device_count() == 1:\n",
        "    print(\"Run\", \"with one GPU\")\n",
        "    model = model.to(device)\n",
        "  else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "    device_ids = GPUtil.getAvailable(limit = 4)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    model = model.to(device)\n",
        "    model = nn.DataParallel(model, device_ids=device_ids)\n",
        "else:\n",
        "  print(\"Run\", \"with CPU\")\n",
        "  model = model\n",
        "#---------------------------------------------------\n",
        "print (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n",
        "global max_grad_norm\n",
        "max_grad_norm = 1.0\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps\t= len(train_dataloader) * num_epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "optimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPKv5_uHqkSv"
      },
      "outputs": [],
      "source": [
        "\n",
        "#---------------------------------------------------\n",
        "print (\"[INFO] step (9) start fine_tuning\")\n",
        "for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
        "  train_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n",
        "  val_loss, val_acc, val_f1, val_recall, val_precision, f1_sarcastic = evaluate(model, validation_dataloader, criterion)\n",
        "  if not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n",
        "  model.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n",
        "  epoch_eval_results = {\"epoch_num\":int(epoch + 1),\n",
        "          \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var, \"f1_sarcastic\":f1_sarcastic }\n",
        "  with open(report,\"a\") as fOut:\n",
        "    fOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n",
        "    fOut.flush()\n",
        "  #------------------------------------\n",
        "  report_df = pd.read_json(report, orient='records', lines=True)\n",
        "  report_df.sort_values(by=[sortby],ascending=False, inplace=True)\n",
        "  report_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V9ZOdVNXAkk"
      },
      "outputs": [],
      "source": [
        "report_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load pretrained model and test"
      ],
      "metadata": {
        "id": "J6vSAGQ8vuAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------\n",
        "print (\"[INFO] step (10) testing\")"
      ],
      "metadata": {
        "id": "sVcQOOINwPhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_dir = config[\"checkpoint_dir\"]+task_name+\"_bert_ckpt/\"\n",
        "report = ckpt_dir+task_name+\"_test_report.tsv\"\n",
        "sorted_report = ckpt_dir+task_name+\"_test_report_sorted.tsv\"\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.mkdir(ckpt_dir)"
      ],
      "metadata": {
        "id": "POLbxzKNSPAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls /gdrive/MyDrive/Master/Dataset/isarcasticisarcastic_MARBERT_bert_ckpt/model_7"
      ],
      "metadata": {
        "id": "THfSnOs1Ni5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config={\"task_name\": \"isarcastic_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"checkpoint_dir\": \"/gdrive/MyDrive/Master/Dataset/isarcastic\",\n",
        "             \"train_file\": \"isarcastic/isarcastic_Ar_train.csv\", #train file path\n",
        "             \"dev_file\": \"isarcastic/task_A_AR_test.csv\", #dev file path or test file path\n",
        "            # \"dev_file\": \"isarcastic/isarcastic_Ar_test2.csv\", #dev file path or test file path\n",
        "\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"fine_tuned_model_path\" : \"/gdrive/MyDrive/Master/Dataset/isarcasticisarcastic_MARBERT_bert_ckpt/model_7\",\n",
        "             \"epochs\": 5, #number of epochs\n",
        "            #  \"content_col\": \"tweet\", #text column\n",
        "             \"content_col\": \"text\",\n",
        "             \"label_col\": \"sarcastic\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 64, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ],
      "metadata": {
        "id": "Dmw9LuAvckAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_file = config[\"data_dir\"]+config[\"dev_file\"]\n",
        "content_col = config[\"content_col\"]"
      ],
      "metadata": {
        "id": "_nE7A7E4c_wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = config['fine_tuned_model_path']"
      ],
      "metadata": {
        "id": "wWctpHeSyImj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/gdrive/MyDrive/Master/Dataset/isarcasticisarcastic_MARBERT_bert_ckpt/model_4/vocab.txt' '/gdrive/MyDrive/Master/Dataset/isarcasticisarcastic_MARBERT_bert_ckpt/model_7/vocab.txt'"
      ],
      "metadata": {
        "id": "MZoIL6h7KC_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"loading BERT setting\")\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "x3Qj1NFIyaiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def data_prepare_BERT(file_path, lab2ind, tokenizer, content_col, label_col, MAX_LEN):\n",
        "  # Use pandas to load dataset\n",
        "  df = pd.read_csv(file_path, delimiter=',', header=0)\n",
        "  df = df[df[content_col].notnull()]\n",
        "  print(\"Data size \", df.shape)\n",
        "  sentences = df[content_col].values\n",
        "  sentences = [\"[CLS] \" + sentence+ \" [SEP]\" for sentence in sentences]\n",
        "  print (\"The first sentence:\")\n",
        "  print (sentences[0])\n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "  print (\"Tokenize the first sentence:\")\n",
        "  print (tokenized_texts[0])\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  print (\"Index numbers of the first sentence:\")\n",
        "  print (input_ids[0])\n",
        "  pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "  print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "  attention_masks = []\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [int(i > 0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "  inputs = torch.tensor(input_ids)\n",
        "  masks = torch.tensor(attention_masks)\n",
        "  return inputs, masks\n"
      ],
      "metadata": {
        "id": "3IFePNWMn5Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_file= \"isarcastic/task_A_AR_test.csv\" "
      ],
      "metadata": {
        "id": "Fp7FSoLQ97no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_col =\"text\""
      ],
      "metadata": {
        "id": "BqTsvP5B-Dmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_inputs, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n",
        "#--------------------------------------\n",
        "print (\"[INFO] step (6') Create an iterator of data with torch DataLoader.\")"
      ],
      "metadata": {
        "id": "3ovvGvSXobJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = TensorDataset(validation_inputs, validation_masks)\n"
      ],
      "metadata": {
        "id": "XdeE_C-vofKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#---------------------------\n",
        "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
        "#------------------------------------------"
      ],
      "metadata": {
        "id": "5ECuMU8Foi5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print (\"[INFO] step (7') run with parallel GPUs\")\n",
        "if torch.cuda.is_available():\n",
        "  if torch.cuda.device_count() == 1:\n",
        "    print(\"Run\", \"with one GPU\")\n",
        "    model = model.to(device)\n",
        "  else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "    device_ids = GPUtil.getAvailable(limit = 4)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    model = model.to(device)\n",
        "    model = nn.DataParallel(model, device_ids=device_ids)\n",
        "else:\n",
        "  print(\"Run\", \"with CPU\")\n",
        "  model = model\n",
        "#---------------------------------------------------\n",
        "print (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n",
        "global max_grad_norm\n",
        "max_grad_norm = 1.0\n",
        "warmup_proportion = 0.1\n",
        "optimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False) \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "MOtiXWZ5NxS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  all_pred=[]\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      input_ids, input_mask = batch\n",
        "      outputs = model(input_ids, input_mask)\n",
        "      logits = outputs[0]\n",
        "      del batch, input_ids, input_mask\n",
        "      probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "      all_pred.extend(predicted)\n",
        "  return all_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "LD0QZWVuotk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 6"
      ],
      "metadata": {
        "id": "QhZiv6PaLcOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_all_pred = evaluate(model, validation_dataloader, criterion)"
      ],
      "metadata": {
        "id": "YU6HGZBjwWMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textfile = open(\"task_a_ar.txt\", \"w\")\n",
        "textfile.write(\"task_a_ar\" + \"\\n\")\n",
        "for i in test_all_pred:\n",
        "    textfile.write(str(i.item()) + \"\\n\")\n",
        "textfile.close()"
      ],
      "metadata": {
        "id": "WMOFK-OesR-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp    \"task_a_ar.txt\" /gdrive/MyDrive/Master/Dataset/Outputs/task_a_ar.txt"
      ],
      "metadata": {
        "id": "szd03QSKuaAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "iSarcasmEval-Marbert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}